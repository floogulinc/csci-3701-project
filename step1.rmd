---
title: "CSCI 3701 Project Step 1"
author: Paul Friederichsen
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)  
library(rvest)
library(rlist)
library(furrr)
```

## Purpose

I found a website called "Mathematics Genealogy Project" and it seems like an intersting set of data to work with. I plan to scrape all the data from the site and get it into a form that it can be explored. There are a number of things that could be interesting about it. 
- How the number of students advised has changed over time
- Demographic changes over time, like countries
- Looking at the "descendants" of specific famous mathematicians

## The Data

My data comes from the [Mathematics Genealogy Project](https://www.genealogy.math.ndsu.nodak.edu/) at NDSU. It is a database of people who have received as doctorate in mathematics, which includes:

- An ID of the person (integer) (this is a property o the URL)
- Their complete name (string)
- Their degrees/dissertations (will be represented as a tibble). For each university/dissertation:
  - The name of the university (string)
  - The year(s) in which their degree was awarded (integer/array of integers)
  - The complete title of the disseration (string)
  - The complete names of their advisors (array of strings)
  - Advisors are also linked to so there will be the ID of each advisor (integers)

They have collected this data from a variety of historical records and data submitted to them. It is presented on a fairly antiquated PHP website with an individual page for each mathematician that are (luckily) given incremental IDs.

From their [FAQ](https://www.genealogy.math.ndsu.nodak.edu/faq.php):

<blockquote style="font-size: 14px">
<b>Where do you get your data?</b></p>

<p>We depend on information from our visitors for most of our data. In cases of partial information, we search Dissertation Abstracts International in an effort to find complete information. We have also entered a considerable amount of data found on lists of graduates maintained by individual departments.</p>
</blockquote>


## Getting the Data

### Grabbing the site

The first step in getting the data is downloading the page for every mathematician. They all use URLs of the format `https://www.genealogy.math.ndsu.nodak.edu/id.php?id=55054` and at the time I downloaded them I determined there were 253772 records based on the number given on the home page and some testing (it was slightly higher than the given number). 


I used curl to go through and download all of the pages:
```bash
curl -OZ https://www.genealogy.math.ndsu.nodak.edu/id.php?id=[1-253772]
```

After that finished (and it had to be restarted a few times), it looks like it missed a few.
```bash
[fried701@csci-4409 site-dump-1]$ ls -l | wc -l
253760
```


So I wrote a small script to find them

```bash
[fried701@csci-4409 site-dump-1]$ seq 253772 | while read -r i; do
>     [[ -f "id.php?id=$i" ]] || echo "$i is missing"
> done                                                                                                                                           
3938 is missing
18047 is missing
32161 is missing
46276 is missing
60392 is missing
74507 is missing
88623 is missing
102739 is missing
116854 is missing
130970 is missing
145086 is missing
159201 is missing
173317 is missing
```

I realized I could use the same code to download the missing files

```bash
seq 253772 | while read -r i; do
    [[ -f "id.php?id=$i" ]] || curl -O https://www.genealogy.math.ndsu.nodak.edu/id.php?id=$i
done
```
Now I had all 253773 files totaling 3.2gb. Though compressed into tar.gz it is 79mb.

### Processing the raw data

Now that I had the dump of all the HTML files, I had to go through them and get it into a useable form.

**Note**: The filenames have changed and all `?` have been replaced with `_`. This is because I extracted them on Windows and NTFS does not allow `?` in filenames

#### Dealing with blank/error pages

For some reason ID `187430` was an empty file and had to be redownloaded. I then noticed multiple files were like that and wrote something to find them:

```{r, eval=FALSE}
index <- 1:253772
index.blanks <- list.filter(index, class(read_html(paste("site-dump-1/id.php_id=", ., sep=""))) == "xml_document")
```

That took about 25 min and found an additional 4 with the same issue.

```
[1] 201546 215662 229777 243846
```

I then went and redownloaded each of the missing ones.

They are not in the archive at the moment, but included in an additional directory called `site-dump-1-patch`.

---

Next, some IDs have a blank page which simply say "You have specified an ID that does not exist in the database. Please back up and try again." So I needed to filter those out.

```{r, eval=FALSE}
index <- 1:253772
index.nomissing <- list.filter(index, class(html_node(read_html(paste("site-dump-1/id.php_id=", ., sep="")), "#paddingWrapper")) != "xml_missing")

saveRDS(index.nomissing, "indexnomissing.Rda")
```

This took 55 min and resulted in 249371 items, meaning there were 4401 bad IDs.

#### Converting to a dataframe/tibble

The next step was to go through all the pages again and make a tibble with the information from them.

**Note**: while working on this part I encountered rare instances of odd data. For example, there are some people with no country flag, or just one while they have multiple universities. I additionally found [a case](https://www.genealogy.math.ndsu.nodak.edu/id.php?id=252229) of someone where for a single dissertation there are two flags. Because of this I had to switch from a "country" field in the inner schools tibble to a "countries" field that is a list.

Another extremly rare one that was difficult to figure out was [the instance](https://www.genealogy.math.ndsu.nodak.edu/id.php?id=236498) of "Ph.D. advisor:" instead of the normal "Advisor" in the text where the advisor links are. In [another instance](https://www.genealogy.math.ndsu.nodak.edu/id.php?id=93048) it was "Supervisor" instead. One of [this person's](https://www.genealogy.math.ndsu.nodak.edu/id.php?id=127962) is "Doctoral advisor". I finally generalized the xpath for the advisor `p` tag with it's style, while keeping the search for "Advisor" because some pages with no advisor don't have the same style for that `p` tag.

```{r, eval=FALSE}
parse.id = function(addr) {
  return(sub(".*id.php\\?id=", '', addr) %>% strtoi)
}

process.indices <- function(ind) {
  paste("site-dump-1/id.php_id=", ind, sep = "") %>% map(read_html) %>% map(html_node, "#paddingWrapper") %>% {
    tibble(
      id = ind,
      name = map(., html_node, "h2") %>% map_chr(html_text, trim = TRUE),
      schools = map(., ~ {
        tibble (
          thesis = html_nodes(.x, "#thesisTitle") %>% html_text(trim = TRUE),
          university = html_nodes(.x, xpath = "//span[contains(@style,'#006633')]") %>% html_text(),
          advisors = html_nodes(.x, xpath = '//p[contains(@style, "text-align: center; line-height: 2.75ex")]|//p[text()[contains(.,"Advisor")]]') %>% map(html_nodes, "a") %>% map(html_attr, "href") %>% map(map, parse.id),
          countries = html_nodes(.x, xpath = "//div[contains(@style, 'line-height: 30px; text-align: center; margin-bottom: 1ex')]") %>% map(html_nodes, "img") %>% map(map, html_attr, "title")
        )
      }),
      advisors = map(., html_nodes, xpath = '//p[contains(@style, "text-align: center; line-height: 2.75ex")]|//p[text()[contains(.,"Advisor")]]') %>% map(html_nodes, "a") %>% map(html_attr, "href") %>% map(map, parse.id),
      students.raw = map(., html_node, "table") %>% map_if(~ class(.) != "xml_missing" ,html_table),
      students.id = map(., html_node, "table") %>% map(html_nodes, "a") %>% map(html_attr, "href") %>% map(map, parse.id)
    )
  }
}
```

```{r, eval = FALSE}
chunk2 <- function(x,n) split(x, cut(seq_along(x), n, labels = FALSE)) 

index.nomissing <- readRDS("indexnomissing.Rda")

site.data <- future_map_dfr(chunk2(index.nomissing, 50), process.indices, .progress = TRUE)

saveRDS(site.data, "sitedata.Rda")
```

`future_map_*` allowed me to drastically speed the process up by doing it in parallel. Chunking the indexes seemed to really help with ram usage as it didn't have to load all the HTML into RAM at the same time. It took 25 min for the whole set of 249371 (after removing missing).

```{r}
site.data <- readRDS("sitedata.Rda")
site.data
```